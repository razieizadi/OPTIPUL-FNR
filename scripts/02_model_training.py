# -*- coding: utf-8 -*-
"""02_model_training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11fJjR0UVaEJjgnRgBSiQ-Z4u0kR83q26
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import cm
from scipy.interpolate import griddata
import openpyxl
from openpyxl.styles import Font, PatternFill, Alignment
from google.colab import files
import subprocess
import os
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.inspection import permutation_importance
import pickle
import warnings
warnings.filterwarnings('ignore')

# ===== INSTALL TIMES NEW ROMAN FONT =====
print("Installing Times New Roman font...")
try:
    subprocess.run(['apt-get', 'install', '-y', 'msttcorefonts', '-qq'],
                   stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    # Remove matplotlib font cache
    subprocess.run(['rm', '-rf', os.path.expanduser('~/.cache/matplotlib')],
                   stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    print("âœ“ Font installation complete")
except:
    print("Note: Using default serif font")

# Set font to Times New Roman
plt.rcParams['font.family'] = 'serif'
plt.rcParams['font.serif'] = ['Times New Roman', 'DejaVu Serif', 'Liberation Serif']
plt.rcParams['mathtext.fontset'] = 'stix'

# ===== INPUT PARAMETERS =====
print("=" * 60)
print("PULTRUSION PARAMETRIC STUDY DATA PROCESSOR")
print("=" * 60)

# Get velocity range
vel_min = float(input("\nEnter minimum velocity (m/min): "))
vel_max = float(input("Enter maximum velocity (m/min): "))
vel_step = float(input("Enter velocity step (m/min): "))

# Get temperature range
temp_min = float(input("\nEnter minimum die temperature (Â°C): "))
temp_max = float(input("Enter maximum die temperature (Â°C): "))
temp_step = float(input("Enter temperature step (Â°C): "))

# Generate parameter arrays
velocities = np.arange(vel_min, vel_max + vel_step/2, vel_step)
temperatures = np.arange(temp_min, temp_max + temp_step/2, temp_step)

print(f"\nVelocities: {velocities}")
print(f"Temperatures: {temperatures}")
print(f"Total number of cases: {len(velocities) * len(temperatures)}")

# ===== UPLOAD FILES =====
print("\n" + "=" * 60)
print("Please upload the 'cure' file:")
uploaded_cure = files.upload()
cure_filename = list(uploaded_cure.keys())[0]

print("\nPlease upload the 'temperature' file:")
uploaded_temp = files.upload()
temp_filename = list(uploaded_temp.keys())[0]

# ===== PROCESS CURE DATA =====
print("\n" + "=" * 60)
print("Processing cure data...")

# Read the cure file
cure_data = pd.read_csv(cure_filename, sep=r'\s+', header=None, names=['Axial_Distance', 'Cure_Degree'])

# Determine number of points per case (assuming distance goes from 0 to 1000)
axial_distances = cure_data['Axial_Distance'].values
# Find where the pattern repeats (when distance resets)
reset_indices = np.where(np.diff(axial_distances) < 0)[0] + 1
if len(reset_indices) > 0:
    points_per_case = reset_indices[0]
else:
    points_per_case = len(axial_distances)

print(f"Points per case: {points_per_case}")

# Create organized cure dataframe
cure_organized = pd.DataFrame()
cure_organized['Axial_Distance'] = axial_distances[:points_per_case]

case_idx = 0
for vel in velocities:
    for temp in temperatures:
        start_idx = case_idx * points_per_case
        end_idx = start_idx + points_per_case
        col_name = f'V={vel:.2f}_T={temp:.0f}'
        cure_organized[col_name] = cure_data['Cure_Degree'].values[start_idx:end_idx]
        case_idx += 1

# ===== PROCESS TEMPERATURE DATA =====
print("Processing temperature data...")

# Read the temperature file
temp_data = pd.read_csv(temp_filename, sep=r'\s+', header=None, names=['Axial_Distance', 'Die_Temperature'])

# Create organized temperature dataframe
temp_organized = pd.DataFrame()
temp_organized['Axial_Distance'] = axial_distances[:points_per_case]

case_idx = 0
for vel in velocities:
    for temp in temperatures:
        start_idx = case_idx * points_per_case
        end_idx = start_idx + points_per_case
        col_name = f'V={vel:.2f}_T={temp:.0f}'
        temp_organized[col_name] = temp_data['Die_Temperature'].values[start_idx:end_idx]
        case_idx += 1

# ===== CREATE SUMMARY DATAFRAME =====
print("Creating summary statistics...")

summary_data = []
for vel in velocities:
    for temp in temperatures:
        col_name = f'V={vel:.2f}_T={temp:.0f}'

        # Final cure degree at 1000mm (last value)
        final_cure = cure_organized[col_name].iloc[-1]

        # Maximum temperature along the die
        max_temp = temp_organized[col_name].max()

        summary_data.append({
            'Velocity (m/min)': vel,
            'Die Temperature (Â°C)': temp,
            'Final Cure Degree at 1000mm': final_cure,
            'Maximum Temperature (Â°C)': max_temp
        })

summary_df = pd.DataFrame(summary_data)

# ===== PREPARE ML TRAINING DATA =====
print("\n" + "=" * 60)
print("MACHINE LEARNING MODEL TRAINING")
print("=" * 60)
print("\nPreparing training data...")

# Create comprehensive dataset for ML
ml_data = []
for vel in velocities:
    for temp in temperatures:
        col_name = f'V={vel:.2f}_T={temp:.0f}'

        # Get all axial distances and corresponding values
        for idx in range(len(cure_organized)):
            axial_dist = cure_organized['Axial_Distance'].iloc[idx]
            cure_value = cure_organized[col_name].iloc[idx]
            temp_value = temp_organized[col_name].iloc[idx]

            ml_data.append({
                'Velocity': vel,
                'Die_Temperature': temp,
                'Axial_Distance': axial_dist,
                'Cure_Degree': cure_value,
                'Temperature': temp_value
            })

ml_df = pd.DataFrame(ml_data)

# Prepare features and targets
X = ml_df[['Velocity', 'Die_Temperature', 'Axial_Distance']].values
y_cure = ml_df['Cure_Degree'].values
y_temp = ml_df['Temperature'].values

# Split data (80% train, 20% test)
X_train, X_test, y_cure_train, y_cure_test, y_temp_train, y_temp_test = train_test_split(
    X, y_cure, y_temp, test_size=0.2, random_state=42
)

# Standardize features
scaler_X = StandardScaler()
X_train_scaled = scaler_X.fit_transform(X_train)
X_test_scaled = scaler_X.transform(X_test)

# Standardize targets for better neural network training
scaler_cure = StandardScaler()
scaler_temp = StandardScaler()
y_cure_train_scaled = scaler_cure.fit_transform(y_cure_train.reshape(-1, 1)).ravel()
y_temp_train_scaled = scaler_temp.fit_transform(y_temp_train.reshape(-1, 1)).ravel()

print(f"âœ“ Training samples: {len(X_train)}")
print(f"âœ“ Testing samples: {len(X_test)}")

# ===== TRAIN MODELS FOR CURE DEGREE =====
print("\n" + "-" * 60)
print("Training Cure Degree Prediction Models...")
print("-" * 60)

models_cure = {}

# 1. Neural Network
print("\n1. Neural Network (MLP)...")
nn_cure = MLPRegressor(hidden_layer_sizes=(100, 100, 50), activation='relu',
                       solver='adam', max_iter=1000, random_state=42,
                       early_stopping=True, validation_fraction=0.1)
nn_cure.fit(X_train_scaled, y_cure_train_scaled)
y_cure_pred_nn_scaled = nn_cure.predict(X_test_scaled)
y_cure_pred_nn = scaler_cure.inverse_transform(y_cure_pred_nn_scaled.reshape(-1, 1)).ravel()
models_cure['Neural Network'] = (nn_cure, scaler_X, scaler_cure)

r2_nn = r2_score(y_cure_test, y_cure_pred_nn)
mse_nn = mean_squared_error(y_cure_test, y_cure_pred_nn)
mae_nn = mean_absolute_error(y_cure_test, y_cure_pred_nn)
print(f"   RÂ² = {r2_nn:.6f}, MSE = {mse_nn:.8f}, MAE = {mae_nn:.8f}")

# 2. Random Forest
print("\n2. Random Forest...")
rf_cure = RandomForestRegressor(n_estimators=200, max_depth=20,
                                random_state=42, n_jobs=-1)
rf_cure.fit(X_train, y_cure_train)
y_cure_pred_rf = rf_cure.predict(X_test)
models_cure['Random Forest'] = (rf_cure, None, None)

r2_rf = r2_score(y_cure_test, y_cure_pred_rf)
mse_rf = mean_squared_error(y_cure_test, y_cure_pred_rf)
mae_rf = mean_absolute_error(y_cure_test, y_cure_pred_rf)
print(f"   RÂ² = {r2_rf:.6f}, MSE = {mse_rf:.8f}, MAE = {mae_rf:.8f}")

# 3. Gradient Boosting
print("\n3. Gradient Boosting...")
gb_cure = GradientBoostingRegressor(n_estimators=200, max_depth=7,
                                    learning_rate=0.1, random_state=42)
gb_cure.fit(X_train, y_cure_train)
y_cure_pred_gb = gb_cure.predict(X_test)
models_cure['Gradient Boosting'] = (gb_cure, None, None)

r2_gb = r2_score(y_cure_test, y_cure_pred_gb)
mse_gb = mean_squared_error(y_cure_test, y_cure_pred_gb)
mae_gb = mean_absolute_error(y_cure_test, y_cure_pred_gb)
print(f"   RÂ² = {r2_gb:.6f}, MSE = {mse_gb:.8f}, MAE = {mae_gb:.8f}")

# ===== TRAIN MODELS FOR TEMPERATURE =====
print("\n" + "-" * 60)
print("Training Temperature Prediction Models...")
print("-" * 60)

models_temp = {}

# 1. Neural Network
print("\n1. Neural Network (MLP)...")
nn_temp = MLPRegressor(hidden_layer_sizes=(100, 100, 50), activation='relu',
                       solver='adam', max_iter=1000, random_state=42,
                       early_stopping=True, validation_fraction=0.1)
nn_temp.fit(X_train_scaled, y_temp_train_scaled)
y_temp_pred_nn_scaled = nn_temp.predict(X_test_scaled)
y_temp_pred_nn = scaler_temp.inverse_transform(y_temp_pred_nn_scaled.reshape(-1, 1)).ravel()
models_temp['Neural Network'] = (nn_temp, scaler_X, scaler_temp)

r2_nn_t = r2_score(y_temp_test, y_temp_pred_nn)
mse_nn_t = mean_squared_error(y_temp_test, y_temp_pred_nn)
mae_nn_t = mean_absolute_error(y_temp_test, y_temp_pred_nn)
print(f"   RÂ² = {r2_nn_t:.6f}, MSE = {mse_nn_t:.8f}, MAE = {mae_nn_t:.8f}")

# 2. Random Forest
print("\n2. Random Forest...")
rf_temp = RandomForestRegressor(n_estimators=200, max_depth=20,
                                random_state=42, n_jobs=-1)
rf_temp.fit(X_train, y_temp_train)
y_temp_pred_rf = rf_temp.predict(X_test)
models_temp['Random Forest'] = (rf_temp, None, None)

r2_rf_t = r2_score(y_temp_test, y_temp_pred_rf)
mse_rf_t = mean_squared_error(y_temp_test, y_temp_pred_rf)
mae_rf_t = mean_absolute_error(y_temp_test, y_temp_pred_rf)
print(f"   RÂ² = {r2_rf_t:.6f}, MSE = {mse_rf_t:.8f}, MAE = {mae_rf_t:.8f}")

# 3. Gradient Boosting
print("\n3. Gradient Boosting...")
gb_temp = GradientBoostingRegressor(n_estimators=200, max_depth=7,
                                    learning_rate=0.1, random_state=42)
gb_temp.fit(X_train, y_temp_train)
y_temp_pred_gb = gb_temp.predict(X_test)
models_temp['Gradient Boosting'] = (gb_temp, None, None)

r2_gb_t = r2_score(y_temp_test, y_temp_pred_gb)
mse_gb_t = mean_squared_error(y_temp_test, y_temp_pred_gb)
mae_gb_t = mean_absolute_error(y_temp_test, y_temp_pred_gb)
print(f"   RÂ² = {r2_gb_t:.6f}, MSE = {mse_gb_t:.8f}, MAE = {mae_gb_t:.8f}")

# ===== SAVE BEST MODELS =====
print("\n" + "-" * 60)
print("Saving trained models...")

# Determine best models based on RÂ²
cure_performances = {'Neural Network': r2_nn, 'Random Forest': r2_rf, 'Gradient Boosting': r2_gb}
temp_performances = {'Neural Network': r2_nn_t, 'Random Forest': r2_rf_t, 'Gradient Boosting': r2_gb_t}

best_cure_model_name = max(cure_performances, key=cure_performances.get)
best_temp_model_name = max(temp_performances, key=temp_performances.get)

print(f"âœ“ Best Cure Model: {best_cure_model_name} (RÂ² = {cure_performances[best_cure_model_name]:.6f})")
print(f"âœ“ Best Temperature Model: {best_temp_model_name} (RÂ² = {temp_performances[best_temp_model_name]:.6f})")

# Save all models
with open('models_cure.pkl', 'wb') as f:
    pickle.dump(models_cure, f)

with open('models_temp.pkl', 'wb') as f:
    pickle.dump(models_temp, f)

print("âœ“ Models saved as 'models_cure.pkl' and 'models_temp.pkl'")

# ===== CREATE MODEL PERFORMANCE COMPARISON CHARTS =====
print("\nCreating model performance comparison charts...")

fig, axes = plt.subplots(2, 3, figsize=(20, 12))

# Cure Degree Models
predictions_cure = {
    'Neural Network': y_cure_pred_nn,
    'Random Forest': y_cure_pred_rf,
    'Gradient Boosting': y_cure_pred_gb
}

for idx, (model_name, y_pred) in enumerate(predictions_cure.items()):
    ax = axes[0, idx]
    ax.scatter(y_cure_test, y_pred, alpha=0.5, s=10, edgecolors='k', linewidths=0.5)

    # Perfect prediction line
    min_val = min(y_cure_test.min(), y_pred.min())
    max_val = max(y_cure_test.max(), y_pred.max())
    ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')

    r2 = r2_score(y_cure_test, y_pred)
    mae = mean_absolute_error(y_cure_test, y_pred)

    ax.set_xlabel('Actual Cure Degree', fontsize=12)
    ax.set_ylabel('Predicted Cure Degree', fontsize=12)
    ax.set_title(f'{model_name}\nRÂ² = {r2:.6f}, MAE = {mae:.6f}', fontsize=13)
    ax.legend(fontsize=10)
    ax.grid(True, alpha=0.3)

# Temperature Models
predictions_temp = {
    'Neural Network': y_temp_pred_nn,
    'Random Forest': y_temp_pred_rf,
    'Gradient Boosting': y_temp_pred_gb
}

for idx, (model_name, y_pred) in enumerate(predictions_temp.items()):
    ax = axes[1, idx]
    ax.scatter(y_temp_test, y_pred, alpha=0.5, s=10, edgecolors='k', linewidths=0.5)

    # Perfect prediction line
    min_val = min(y_temp_test.min(), y_pred.min())
    max_val = max(y_temp_test.max(), y_pred.max())
    ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')

    r2 = r2_score(y_temp_test, y_pred)
    mae = mean_absolute_error(y_temp_test, y_pred)

    ax.set_xlabel('Actual Temperature (Â°C)', fontsize=12)
    ax.set_ylabel('Predicted Temperature (Â°C)', fontsize=12)
    ax.set_title(f'{model_name}\nRÂ² = {r2:.6f}, MAE = {mae:.4f}', fontsize=13)
    ax.legend(fontsize=10)
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('ml_model_comparison.png', dpi=600, bbox_inches='tight', facecolor='white')
plt.savefig('ml_model_comparison.pdf', dpi=600, bbox_inches='tight', facecolor='white')
plt.savefig('ml_model_comparison.eps', dpi=600, bbox_inches='tight', facecolor='white')
plt.show()

print("âœ“ Model comparison charts saved")

# ===== FEATURE IMPORTANCE ANALYSIS =====
print("\n" + "-" * 60)
print("Analyzing Feature Importance...")
print("-" * 60)

feature_names = ['Velocity', 'Die Temperature', 'Axial Distance']

# Extract feature importance from tree-based models
# For Cure Degree
rf_cure_importance = rf_cure.feature_importances_
gb_cure_importance = gb_cure.feature_importances_

# For Temperature
rf_temp_importance = rf_temp.feature_importances_
gb_temp_importance = gb_temp.feature_importances_

# Calculate permutation importance for Neural Networks

print("\nCalculating permutation importance for Neural Networks...")
nn_cure_perm = permutation_importance(nn_cure, X_test_scaled,
                                      scaler_cure.transform(y_cure_test.reshape(-1, 1)).ravel(),
                                      n_repeats=10, random_state=42, n_jobs=-1)
nn_temp_perm = permutation_importance(nn_temp, X_test_scaled,
                                      scaler_temp.transform(y_temp_test.reshape(-1, 1)).ravel(),
                                      n_repeats=10, random_state=42, n_jobs=-1)

nn_cure_importance = nn_cure_perm.importances_mean
nn_temp_importance = nn_temp_perm.importances_mean

# Normalize to percentages
nn_cure_importance = nn_cure_importance / nn_cure_importance.sum() * 100
nn_temp_importance = nn_temp_importance / nn_temp_importance.sum() * 100
rf_cure_importance = rf_cure_importance / rf_cure_importance.sum() * 100
gb_cure_importance = gb_cure_importance / gb_cure_importance.sum() * 100
rf_temp_importance = rf_temp_importance / rf_temp_importance.sum() * 100
gb_temp_importance = gb_temp_importance / gb_temp_importance.sum() * 100

# Print feature importance
print("\n" + "=" * 60)
print("FEATURE IMPORTANCE ANALYSIS")
print("=" * 60)

print("\nðŸ“Š CURE DEGREE PREDICTION:")
print("-" * 60)
for model_name, importance in [('Neural Network', nn_cure_importance),
                                ('Random Forest', rf_cure_importance),
                                ('Gradient Boosting', gb_cure_importance)]:
    print(f"\n{model_name}:")
    for feat, imp in zip(feature_names, importance):
        print(f"  {feat:20s}: {imp:6.2f}%")

print("\nðŸ“Š TEMPERATURE PREDICTION:")
print("-" * 60)
for model_name, importance in [('Neural Network', nn_temp_importance),
                                ('Random Forest', rf_temp_importance),
                                ('Gradient Boosting', gb_temp_importance)]:
    print(f"\n{model_name}:")
    for feat, imp in zip(feature_names, importance):
        print(f"  {feat:20s}: {imp:6.2f}%")

# Create feature importance visualization
fig, axes = plt.subplots(2, 3, figsize=(18, 10))

# Cure Degree Feature Importance
importance_data_cure = {
    'Neural Network': nn_cure_importance,
    'Random Forest': rf_cure_importance,
    'Gradient Boosting': gb_cure_importance
}

for idx, (model_name, importance) in enumerate(importance_data_cure.items()):
    ax = axes[0, idx]
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c']
    bars = ax.barh(feature_names, importance, color=colors, edgecolor='black', linewidth=1.5)

    # Add percentage labels
    for bar, imp in zip(bars, importance):
        width = bar.get_width()
        ax.text(width + 1, bar.get_y() + bar.get_height()/2,
                f'{imp:.1f}%', ha='left', va='center', fontsize=11, fontweight='bold')

    ax.set_xlabel('Importance (%)', fontsize=12)
    ax.set_title(f'{model_name}\nCure Degree Prediction', fontsize=13, fontweight='bold')
    ax.set_xlim(0, max(importance) * 1.15)
    ax.grid(axis='x', alpha=0.3)
    ax.tick_params(axis='both', labelsize=11)

# Temperature Feature Importance
importance_data_temp = {
    'Neural Network': nn_temp_importance,
    'Random Forest': rf_temp_importance,
    'Gradient Boosting': gb_temp_importance
}

for idx, (model_name, importance) in enumerate(importance_data_temp.items()):
    ax = axes[1, idx]
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c']
    bars = ax.barh(feature_names, importance, color=colors, edgecolor='black', linewidth=1.5)

    # Add percentage labels
    for bar, imp in zip(bars, importance):
        width = bar.get_width()
        ax.text(width + 1, bar.get_y() + bar.get_height()/2,
                f'{imp:.1f}%', ha='left', va='center', fontsize=11, fontweight='bold')

    ax.set_xlabel('Importance (%)', fontsize=12)
    ax.set_title(f'{model_name}\nTemperature Prediction', fontsize=13, fontweight='bold')
    ax.set_xlim(0, max(importance) * 1.15)
    ax.grid(axis='x', alpha=0.3)
    ax.tick_params(axis='both', labelsize=11)

plt.tight_layout()
plt.savefig('feature_importance.png', dpi=600, bbox_inches='tight', facecolor='white')
plt.savefig('feature_importance.pdf', dpi=600, bbox_inches='tight', facecolor='white')
plt.savefig('feature_importance.eps', dpi=600, bbox_inches='tight', facecolor='white')
plt.show()

print("\nâœ“ Feature importance charts saved")

# Create feature importance summary dataframe
feature_importance_summary = []
for feat_idx, feat_name in enumerate(feature_names):
    feature_importance_summary.append({
        'Feature': feat_name,
        'NN_Cure (%)': nn_cure_importance[feat_idx],
        'RF_Cure (%)': rf_cure_importance[feat_idx],
        'GB_Cure (%)': gb_cure_importance[feat_idx],
        'Avg_Cure (%)': np.mean([nn_cure_importance[feat_idx],
                                  rf_cure_importance[feat_idx],
                                  gb_cure_importance[feat_idx]]),
        'NN_Temp (%)': nn_temp_importance[feat_idx],
        'RF_Temp (%)': rf_temp_importance[feat_idx],
        'GB_Temp (%)': gb_temp_importance[feat_idx],
        'Avg_Temp (%)': np.mean([nn_temp_importance[feat_idx],
                                  rf_temp_importance[feat_idx],
                                  gb_temp_importance[feat_idx]])
    })

feature_importance_df = pd.DataFrame(feature_importance_summary)

# ===== CREATE MODEL PERFORMANCE SUMMARY TABLE =====
performance_data = []

for model_name in ['Neural Network', 'Random Forest', 'Gradient Boosting']:
    if model_name == 'Neural Network':
        cure_pred = y_cure_pred_nn
        temp_pred = y_temp_pred_nn
    elif model_name == 'Random Forest':
        cure_pred = y_cure_pred_rf
        temp_pred = y_temp_pred_rf
    else:
        cure_pred = y_cure_pred_gb
        temp_pred = y_temp_pred_gb

    performance_data.append({
        'Model': model_name,
        'Cure RÂ²': r2_score(y_cure_test, cure_pred),
        'Cure MSE': mean_squared_error(y_cure_test, cure_pred),
        'Cure MAE': mean_absolute_error(y_cure_test, cure_pred),
        'Temp RÂ²': r2_score(y_temp_test, temp_pred),
        'Temp MSE': mean_squared_error(y_temp_test, temp_pred),
        'Temp MAE': mean_absolute_error(y_temp_test, temp_pred)
    })

performance_df = pd.DataFrame(performance_data)

print("\n" + "=" * 60)

# ===== EXPORT TO EXCEL =====
print("Exporting to Excel...")

with pd.ExcelWriter('pultrusion_results.xlsx', engine='openpyxl') as writer:
    # Sheet 1: Summary
    summary_df.to_excel(writer, sheet_name='Summary', index=False)

    # Sheet 2: ML Model Performance
    performance_df.to_excel(writer, sheet_name='ML_Model_Performance', index=False)

    # Sheet 3: Feature Importance
    feature_importance_df.to_excel(writer, sheet_name='Feature_Importance', index=False)

    # Sheet 4: Cure Data
    cure_organized.to_excel(writer, sheet_name='Cure_Degree', index=False)

    # Sheet 5: Temperature Data
    temp_organized.to_excel(writer, sheet_name='Die_Temperature', index=False)

# Format the Excel file
wb = openpyxl.load_workbook('pultrusion_results.xlsx')
ws_summary = wb['Summary']

# Format headers
header_fill = PatternFill(start_color='366092', end_color='366092', fill_type='solid')
header_font = Font(bold=True, color='FFFFFF')

for cell in ws_summary[1]:
    cell.fill = header_fill
    cell.font = header_font
    cell.alignment = Alignment(horizontal='center', vertical='center')

# Auto-adjust column widths
for column in ws_summary.columns:
    max_length = 0
    column = [cell for cell in column]
    for cell in column:
        try:
            if len(str(cell.value)) > max_length:
                max_length = len(cell.value)
        except:
            pass
    adjusted_width = (max_length + 2)
    ws_summary.column_dimensions[column[0].column_letter].width = adjusted_width

wb.save('pultrusion_results.xlsx')

print("Excel file 'pultrusion_results.xlsx' created successfully!")

# ===== CREATE DESIGN CHARTS =====
print("\nCreating high-resolution design charts for publication...")

# Prepare data for contour plots
vel_grid, temp_grid = np.meshgrid(velocities, temperatures)

# Reshape data for plotting
cure_matrix = summary_df['Final Cure Degree at 1000mm'].values.reshape(len(velocities), len(temperatures)).T
temp_max_matrix = summary_df['Maximum Temperature (Â°C)'].values.reshape(len(velocities), len(temperatures)).T

# Determine contour levels for cure degree (0.001 step)
cure_min = cure_matrix.min()
cure_max = cure_matrix.max()
cure_levels_contour = np.arange(np.floor(cure_min * 1000) / 1000,
                                 np.ceil(cure_max * 1000) / 1000 + 0.001,
                                 0.001)

# Create figure with two subplots
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 7))

# Plot 1: Final Cure Degree
contour1 = ax1.contourf(vel_grid, temp_grid, cure_matrix, levels=50, cmap='RdYlGn')
contour1_lines = ax1.contour(vel_grid, temp_grid, cure_matrix, levels=cure_levels_contour,
                              colors='black', linewidths=0.8, alpha=0.6)
ax1.clabel(contour1_lines, inline=True, fontsize=9, fmt='%.3f', inline_spacing=8)
cbar1 = plt.colorbar(contour1, ax=ax1, format='%.3f')
cbar1.set_label('Final Cure Degree', rotation=270, labelpad=25, fontsize=13)
cbar1.ax.tick_params(labelsize=11)
ax1.set_xlabel('Velocity (m/min)', fontsize=14)
ax1.set_ylabel('Die Temperature (Â°C)', fontsize=14)
ax1.set_title('Final Cure Degree at 1000mm', fontsize=15, pad=20)
ax1.tick_params(axis='both', which='major', labelsize=12)
ax1.grid(True, alpha=0.2, linestyle='--', linewidth=0.5)

# Plot 2: Maximum Temperature
contour2 = ax2.contourf(vel_grid, temp_grid, temp_max_matrix, levels=50, cmap='hot_r')
contour2_lines = ax2.contour(vel_grid, temp_grid, temp_max_matrix, levels=15,
                              colors='black', linewidths=0.8, alpha=0.6)
ax2.clabel(contour2_lines, inline=True, fontsize=9, fmt='%.1f', inline_spacing=8)
cbar2 = plt.colorbar(contour2, ax=ax2, format='%.1f')
cbar2.set_label('Maximum Temperature (Â°C)', rotation=270, labelpad=25, fontsize=13)
cbar2.ax.tick_params(labelsize=11)
ax2.set_xlabel('Velocity (m/min)', fontsize=14)
ax2.set_ylabel('Die Temperature (Â°C)', fontsize=14)
ax2.set_title('Maximum Temperature Along Die', fontsize=15, pad=20)
ax2.tick_params(axis='both', which='major', labelsize=12)
ax2.grid(True, alpha=0.2, linestyle='--', linewidth=0.5)

plt.tight_layout()
plt.savefig('design_charts_publication.png', dpi=600, bbox_inches='tight', facecolor='white')
plt.savefig('design_charts_publication.pdf', dpi=600, bbox_inches='tight', facecolor='white')
plt.savefig('design_charts_publication.eps', dpi=600, bbox_inches='tight', facecolor='white')
plt.show()

print("High-resolution design charts saved:")
print("  - design_charts_publication.png (600 dpi)")
print("  - design_charts_publication.pdf (vector)")
print("  - design_charts_publication.eps (vector)")

# ===== CREATE 3D SURFACE PLOTS =====
print("\nCreating high-resolution 3D surface plots...")

fig = plt.figure(figsize=(18, 7))

# 3D plot 1: Final Cure Degree
ax1 = fig.add_subplot(121, projection='3d')
surf1 = ax1.plot_surface(vel_grid, temp_grid, cure_matrix, cmap='RdYlGn',
                          edgecolor='none', alpha=0.95, antialiased=True,
                          linewidth=0, shade=True)
ax1.set_xlabel('Velocity (m/min)', fontsize=13, labelpad=10)
ax1.set_ylabel('Die Temperature (Â°C)', fontsize=13, labelpad=10)
ax1.set_zlabel('Final Cure Degree', fontsize=13, labelpad=10)
ax1.set_title('Final Cure Degree at 1000mm', fontsize=15, pad=20)
ax1.tick_params(axis='both', which='major', labelsize=11)
ax1.view_init(elev=25, azim=45)
cbar1 = fig.colorbar(surf1, ax=ax1, shrink=0.6, aspect=10, pad=0.1, format='%.3f')
cbar1.ax.tick_params(labelsize=10)

# 3D plot 2: Maximum Temperature
ax2 = fig.add_subplot(122, projection='3d')
surf2 = ax2.plot_surface(vel_grid, temp_grid, temp_max_matrix, cmap='hot_r',
                          edgecolor='none', alpha=0.95, antialiased=True,
                          linewidth=0, shade=True)
ax2.set_xlabel('Velocity (m/min)', fontsize=13, labelpad=10)
ax2.set_ylabel('Die Temperature (Â°C)', fontsize=13, labelpad=10)
ax2.set_zlabel('Maximum Temperature (Â°C)', fontsize=13, labelpad=10)
ax2.set_title('Maximum Temperature Along Die', fontsize=15, pad=20)
ax2.tick_params(axis='both', which='major', labelsize=11)
ax2.view_init(elev=25, azim=45)
cbar2 = fig.colorbar(surf2, ax=ax2, shrink=0.6, aspect=10, pad=0.1, format='%.1f')
cbar2.ax.tick_params(labelsize=10)

plt.tight_layout()
plt.savefig('design_charts_3d_publication.png', dpi=600, bbox_inches='tight', facecolor='white')
plt.savefig('design_charts_3d_publication.pdf', dpi=600, bbox_inches='tight', facecolor='white')
plt.savefig('design_charts_3d_publication.eps', dpi=600, bbox_inches='tight', facecolor='white')
plt.show()

print("High-resolution 3D charts saved:")
print("  - design_charts_3d_publication.png (600 dpi)")
print("  - design_charts_3d_publication.pdf (vector)")
print("  - design_charts_3d_publication.eps (vector)")

# ===== SUMMARY =====
print("\n" + "=" * 60)
print("PROCESSING COMPLETE!")
print("=" * 60)
print("\nGenerated files:")
print("\nðŸ“Š Excel File:")
print("  â€¢ pultrusion_results.xlsx (includes ML performance & feature importance)")
print("\nðŸ“ˆ 2D Contour Plots (Publication Quality - 600 DPI):")
print("  â€¢ design_charts_publication.png")
print("  â€¢ design_charts_publication.pdf (vector)")
print("  â€¢ design_charts_publication.eps (vector)")
print("\nðŸ“‰ 3D Surface Plots (Publication Quality - 600 DPI):")
print("  â€¢ design_charts_3d_publication.png")
print("  â€¢ design_charts_3d_publication.pdf (vector)")
print("  â€¢ design_charts_3d_publication.eps (vector)")
print("\nðŸ¤– ML Model Performance:")
print("  â€¢ ml_model_comparison.png")
print("  â€¢ ml_model_comparison.pdf (vector)")
print("  â€¢ ml_model_comparison.eps (vector)")
print("\nðŸŽ¯ Feature Importance Analysis:")
print("  â€¢ feature_importance.png")
print("  â€¢ feature_importance.pdf (vector)")
print("  â€¢ feature_importance.eps (vector)")
print("\nðŸ’¾ Trained Models:")
print("  â€¢ models_cure.pkl (trained models)")
print("  â€¢ models_temp.pkl (trained models)")
print("\n" + "=" * 60)
print("Files are ready in your Colab environment.")
print("Access them from the Files panel on the left.")
print("PDF and EPS formats are vector graphics, ideal for journals.")
print("=" * 60)

# ===== PREDICTION FUNCTION =====
def predict_pultrusion(velocity, die_temperature, axial_distance,
                       cure_model_name='best', temp_model_name='best'):
    """
    Predict cure degree and temperature for given operating conditions.

    Parameters:
    -----------
    velocity : float
        Pulling velocity in m/min
    die_temperature : float
        Die temperature in Â°C
    axial_distance : float or array-like
        Axial distance(s) in mm
    cure_model_name : str
        Model to use: 'Neural Network', 'Random Forest', 'Gradient Boosting', or 'best'
    temp_model_name : str
        Model to use: 'Neural Network', 'Random Forest', 'Gradient Boosting', or 'best'

    Returns:
    --------
    cure_prediction : float or array
        Predicted cure degree
    temp_prediction : float or array
        Predicted temperature in Â°C
    """

    # Use best models if specified
    if cure_model_name == 'best':
        cure_model_name = best_cure_model_name
    if temp_model_name == 'best':
        temp_model_name = best_temp_model_name

    # Prepare input
    if np.isscalar(axial_distance):
        X_input = np.array([[velocity, die_temperature, axial_distance]])
    else:
        X_input = np.array([[velocity, die_temperature, d] for d in axial_distance])

    # Get models
    cure_model, cure_scaler_X, cure_scaler_y = models_cure[cure_model_name]
    temp_model, temp_scaler_X, temp_scaler_y = models_temp[temp_model_name]

    # Predict cure degree
    if cure_scaler_X is not None:
        X_scaled = cure_scaler_X.transform(X_input)
        cure_pred_scaled = cure_model.predict(X_scaled)
        cure_prediction = cure_scaler_y.inverse_transform(cure_pred_scaled.reshape(-1, 1)).ravel()
    else:
        cure_prediction = cure_model.predict(X_input)

    # Predict temperature
    if temp_scaler_X is not None:
        X_scaled = temp_scaler_X.transform(X_input)
        temp_pred_scaled = temp_model.predict(X_scaled)
        temp_prediction = temp_scaler_y.inverse_transform(temp_pred_scaled.reshape(-1, 1)).ravel()
    else:
        temp_prediction = temp_model.predict(X_input)

    if np.isscalar(axial_distance):
        return cure_prediction[0], temp_prediction[0]
    else:
        return cure_prediction, temp_prediction

print("\n" + "=" * 60)
print("INTERACTIVE PREDICTION TOOL")
print("=" * 60)
print("\nYou can now make predictions using the trained models!")
print("Use the predict_pultrusion() function:")
print("\nExample usage:")
print("  cure, temp = predict_pultrusion(velocity=0.15, die_temperature=125, axial_distance=500)")
print("  print(f'Predicted Cure: {cure:.4f}, Temperature: {temp:.2f}Â°C')")
print("\nFor multiple axial distances:")
print("  distances = np.linspace(0, 1000, 100)")
print("  cure_profile, temp_profile = predict_pultrusion(0.15, 125, distances)")
print("\nYou can specify which model to use:")
print("  cure, temp = predict_pultrusion(0.15, 125, 500, ")
print("                                  cure_model_name='Neural Network',")
print("                                  temp_model_name='Random Forest')")
print("\n" + "=" * 60)

# ===== INTERACTIVE PREDICTION DEMO =====
print("\n" + "=" * 60)
print("DEMONSTRATION: Prediction Example")
print("=" * 60)

demo_vel = (velocities[0] + velocities[-1]) / 2
demo_temp = (temperatures[0] + temperatures[-1]) / 2
demo_dist = 500

cure_pred, temp_pred = predict_pultrusion(demo_vel, demo_temp, demo_dist)

print(f"\nInput Conditions:")
print(f"  Velocity: {demo_vel:.2f} m/min")
print(f"  Die Temperature: {demo_temp:.0f}Â°C")
print(f"  Axial Distance: {demo_dist:.0f} mm")
print(f"\nPredicted Results (using best models):")
print(f"  Cure Degree: {cure_pred:.6f}")
print(f"  Temperature: {temp_pred:.2f}Â°C")

# Create prediction profile plot
print("\nCreating prediction profile example...")
distances_demo = np.linspace(0, 1000, 200)
cure_profile, temp_profile = predict_pultrusion(demo_vel, demo_temp, distances_demo)

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

ax1.plot(distances_demo, cure_profile, 'b-', linewidth=2, label='Predicted')
ax1.set_xlabel('Axial Distance (mm)', fontsize=13)
ax1.set_ylabel('Cure Degree', fontsize=13)
ax1.set_title(f'Cure Profile Prediction\nV={demo_vel:.2f} m/min, T={demo_temp:.0f}Â°C', fontsize=14)
ax1.grid(True, alpha=0.3)
ax1.legend(fontsize=11)

ax2.plot(distances_demo, temp_profile, 'r-', linewidth=2, label='Predicted')
ax2.set_xlabel('Axial Distance (mm)', fontsize=13)
ax2.set_ylabel('Temperature (Â°C)', fontsize=13)
ax2.set_title(f'Temperature Profile Prediction\nV={demo_vel:.2f} m/min, T={demo_temp:.0f}Â°C', fontsize=14)
ax2.grid(True, alpha=0.3)
ax2.legend(fontsize=11)

plt.tight_layout()
plt.savefig('prediction_example.png', dpi=600, bbox_inches='tight', facecolor='white')
plt.savefig('prediction_example.pdf', dpi=600, bbox_inches='tight', facecolor='white')
plt.show()

print("âœ“ Prediction example saved")
print("\n" + "=" * 60)